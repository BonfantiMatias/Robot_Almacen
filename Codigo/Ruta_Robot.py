# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NXKXr2KdTanC1o5YSPb-hD1MOe_P2iR9

#Importar Librerias
"""

import numpy as np

"""#Configuracion de los parametros gamma y alpha para el algoritmo de Q-Learning"""

gamma = 0.75
alpha = 0.9

"""#Definicion del Entorno

##Definicion de Estados
"""

localizacion_estado = {"A": 0,
                     "B": 1,
                     "C": 2,
                     "D": 3,
                     "E": 4,
                     "F": 5,
                     "G": 6,
                     "H": 7,
                     "I": 8,
                     "J": 9,
                     "K": 10,
                     "L": 11}

"""##Definicion de Acciones"""

acciones = [0,1,2,3,4,5,6,7,8,9,10,11]

"""##Definicion de las recompensas"""

#Columnas      A,B,C,D,E,F,G,H,I,J,K,L
R = np.array([[0,1,0,0,0,0,0,0,0,0,0,0], # A
              [1,0,1,0,0,1,0,0,0,0,0,0], # B
              [0,1,0,0,0,0,1,0,0,0,0,0], # C
              [0,0,0,0,0,0,0,1,0,0,0,0], # D
              [0,0,0,0,0,0,0,0,1,0,0,0], # E
              [0,1,0,0,0,0,0,0,0,1,0,0], # F
              [0,0,1,0,0,0,1,1,0,0,0,0], # G
              [0,0,0,1,0,0,1,0,0,0,0,1], # H
              [0,0,0,0,1,0,0,0,0,1,0,0], # I
              [0,0,0,0,0,1,0,0,1,0,1,0], # J
              [0,0,0,0,0,0,0,0,0,1,0,1], # K
              [0,0,0,0,0,0,0,1,0,0,1,0]])# L

"""#Construccion de la solucion de IA con Q-Learning

##Transformacion inversa de estados a ubicaciones
"""

estado_localizacion = {estado: localizacion for localizacion, estado in localizacion_estado.items()}

"""## Crear la funcion final que nos devuelva la ruta optima"""

def ruta(localizacion_inicial, localizacion_final):
    R_new = np.copy(R)
    estado_final = localizacion_estado[localizacion_final]
    R_new[estado_final, estado_final] = 1000
    
    Q = np.array(np.zeros([12, 12]))
    for i in range(1000):
        estado_actual = np.random.randint(0, 12)
        movimientos_posibles = []
        for j in range(12):
            if R_new[estado_actual, j] > 0:
                movimientos_posibles.append(j)
        siguiente_estado = np.random.choice(movimientos_posibles)
        TD = R_new[estado_actual, siguiente_estado] + gamma*Q[siguiente_estado, np.argmax(Q[siguiente_estado,])] - Q[estado_actual, siguiente_estado]
        Q[estado_actual, siguiente_estado] = Q[estado_actual, siguiente_estado] + alpha*TD
    
    route = [localizacion_inicial]
    siguiente_localizacion = localizacion_inicial
    while(siguiente_localizacion != localizacion_final):
        estado_inicial = localizacion_estado[localizacion_inicial]
        siguiente_estado = np.argmax(Q[estado_inicial, ])
        siguiente_localizacion = estado_localizacion[siguiente_estado]
        route.append(siguiente_localizacion)
        localizacion_inicial = siguiente_localizacion
    return route

"""# Poner el Modelo en Produccion"""

def mejor_ruta(localizacion_inicial, localizacion_intermedia, localizacion_final):
    return ruta(localizacion_inicial, localizacion_intermedia) + ruta(localizacion_intermedia, localizacion_final)[1:]

# Imprimir la ruta final
print("Ruta Elegida:")
print(mejor_ruta('E', 'A', 'D'))